# Image Captioning

- USe pretrained network like VGG16 as a feature extractor or encoder to get
    feature vector of the input image. The feature vector is then used as input
    to RNN after some processing steps (generally an additional linear layer
    before feeding to the RNN.)
- CNN - RNN Model. Output of CNN connected as input to RNN. 
- CNN - Encoder, RNN- Decoder
- use a combination of spatial observations and sequential text descriptions to write
    a caption
- COCO Dataset - Common Objects in COntext: contains a large variety of images.
    Each image has five associated captions
- RNN decodes the feature vector and turn it into natural language.  

## Video captioning
- similar. The only difference is a series of input frames (images) passed to
    CNN, which outputs a series of feature vectors.
- These feature vectors are combined to form a single feature vector (generally
    by taking an average) and then fed to the RNN. 
- The remaining steps are same as image captioning.

